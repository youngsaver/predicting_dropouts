---
title: "Predictive Analytics and Machine Learning"
author: "Dashiell Young-Saver, Jared Knowles"
date: "Jun 30, 2018 (Replace with date uploaded to OpenSDP)"
output: 
  html_document:
    theme: simplex
    css: ../docs/styles.css
    highlight: NULL
    keep_md: true
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

# Predicting students who will dropout
*Using machine learning to create an early warning system of predicted dropouts*

*Programmed in R*

## Getting Started

```{r knitrSetup, echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, comment=NA}
# Set options for knitr
library(knitr)
knitr::opts_chunk$set(comment=NA, warning=FALSE, echo=TRUE,
                      root.dir = normalizePath("../"),
                      error=FALSE, message=FALSE, fig.align='center',
                      fig.width=8, fig.height=6, dpi = 144, 
                      fig.path = "../figure/E_", 
                      cache.path = "../cache/E_")
options(width=80)
```

<div class="navbar navbar-default navbar-fixed-top" id="logo">
<div class="container">
<img src="../img/open_sdp_logo_red.png" style="display: block; margin: 0 auto; height: 115px;">
</div>
</div>

For this guide, the user will need R, Rstudio, and the following helpful R 
packages:

- `dplyr`: For manipulating variables and tidying output
- `caret`: For cross-validation and learning algorithms
- `glmnet`: For implementing the Lasso
- `plotROC`: For plotting ROC curves
- `doFuture`: For parallel processing
- `foreach`: For loop structure with parallel processing

Here is some code to install them:

```{r installPackages, eval=FALSE}
# Install add-on packages needed
install.packages("dplyr")
install.packages("caret")
install.packages("glmnet")
install.packages("plotROC")
install.packages("doFuture") 
install.packages("foreach")

```

### Objective

In this guide, you will be able to use techniques in machine learning to tune, 
fit, and use a model that predicts likely dropouts in a school system.

### Using this Guide

This guide utilizes synthetically-generated data, designed to roughly match data
put out by the state of Kentucky. However, it can be modified to fit data from 
any education context.

Once you have identified analyses that you want to try to replicate or modify, 
click the "Download" buttons to download R code and sample data. You can make 
changes to the charts using the code and sample data, or modify the code to 
work with your own data. If you are familiar with GitHub, you can click "Go 
to Repository" and clone the entire repository to your own computer. 

Go to the Participate page to read about more ways to engage with the OpenSDP 
community or reach out for assistance in adapting this code for your specific 
context.

### About the Data

The synthetically-generated data used in the guide mimics a 2015 dataset 
tracking the academic progress of Kentucky students who matriculated to the 9th 
grade in 2008. Its rows are student-level, and it contains many features. 
However, we are assuming that the state of "Faketucky" would only be using 
certain features to predict future dropouts. Here are the relevant predictor 
features (and outcome feature) that we retain from the dataset for model fitting
and analysis: 

| Feature name          | Feature Description                                                                 |
|:------           |:-----------------------------------------                                                                       |
| `dropout`             | Outcome indicator: "0" if did not dropout of high school, 1 if did                  |
| `male`                | Indicator: "0" if female, "1" if male                                               |
| `race_ethnicity`      | Categorical: student race-ethnicity                                                 |
| `math_ss`  | Numeric: 8th grade standardized math exam scale score                               |
| `read_ss`  | Numeric: 8th grade standardized reading exam scale score                            |
| `gifted`           | Indicator: "0" if not enrolled in gifted program, "1" if enrolled     |
| `ever_alternative`     | Indicator: "0" if never in alternative school, "1" if ever enrolled   |
| `iep`             | Indicator: "0" if not enrolled in special education, "1" if enrolled  |
| `pct_days_absent`       | Numeric: Percent of school days absent                                |
| `gpa`          | Numeric: Cumulative GPA                                               |
| `frpl`             | Indicator: "0" if not enrolled in free or reduced lunch program, "1" if enrolled |
| `ell`              | Indicator: "0" if not enrolled in English language learner program, "1" if enrolled |

A manual with a full description of the data can be found in the `man` folder of
the GitHub repository.

## Introduction

This guide demonstrates an introductory machine learning approach to 
developing a model for predicting if a student will drop out of high school.
Such models can provide "early warning systems" for students identified as
likely to dropout, allowing policymakers to develop interventions that target
those students.

In this scenario, we are developing a predictive dropout model for the state
of Faketucky. The state would like a model that takes as input student-level
measurements of the features described in the table above and outputs a prediction
of whether or not the student will drop out of high school. The state's thinking
is that if it can identify likely dropouts using data collected early in a student's
middle and high school career, it can introduce targeted interventions to prevent
those students from dropping out. However, these interventions are costly and cannot
be given to most students. To make the benefits outweigh the costs, the model must
not only accurately predict who will drop out, but it also must avoid "false alarms", i.e.,
identifying students as targets for intervention who would not have dropped out anyway.

This is a standard binary classification problem: we are predicting if a student will
fall into one category (a dropout) or its opposite (a non-dropout). The student must
fall within one of those two categories. There are many techniques one could use to
do such a prediction, some more complex than others. (Many of the best
predictive analytics packages are written in the R programming language.) Here, we 
demonstrate how to do a type of modified logistic regression: a Lasso regression. 
We chose the Lasso because it lends itself to demonstration of several key concepts
in machine learning, particularly the idea of "overfitting". It also based on a 
more commonly-known technique: logistic regression. However, many types of models can
be fitted and used to make predictions within the framework provided by this guide, 
particularly in the cross-validation section. 

Here are the steps:

1. explore the data, especially looking for missing data and skewed variables
2. split data between a train and test set
3. process training data based on exploration, to prepare for fitting
4. use cross-validation to train the parameters of the Lasso model
6. process the testing data (based on processing of training data)
7. generate an ROC curve from test set predictions and determine cutpoint
8. report expected accuracies of final model on out-of-sample data

## Setup

Let's call the packages we need as well as some helper functions. In addition, 
we will download the dataset.

```{r loadWorkspace}
# Load the packages you need
library(dplyr)
library(plotROC)
library(caret)
library(glmnet)
library(doFuture)
library(foreach) 
registerDoFuture() # configure futures to work in R


# Load the helper functions not in packages
source("../R/helper_funcs.R")

# Read in the data
# This command assumes that the data is in a folder called data, below your 
# current working directory. You can check your working directory with the 
# getwd() command, and you can set your working directory using the RStudio 
# environment, or the setwd() command.

data <- read.csv("../data/training_2009.csv", stringsAsFactors = FALSE)
```

### Validate the data

Ensure that the data imported correctly by checking that the data is unique by 
student ID.

```{r OneRowPerStudent}
nrow(data) == n_distinct(data$sid)
```

Faketucky wants the model to based only on certain predictors and, of course, 
the outcome variable. We will select for just those predictors and the outcome 
variable here:

```{r Selection}
#Features to be retained in dataset
features <- c("dropout",
              "male",
              "race_ethnicity",
              "math_ss",
              "read_ss",
              "gifted",
              "ever_alternative",
              "iep",
              "pct_days_absent",
              "gpa",
              "frpl",
              "ell")

#Selects features
data <- data[, features]

#Shows dimensions of data
dim(data)
```

Note that we have more than 52,000 student records in this dataset. A lot
of data is required for making accurate prediction models and a dataset
of this size should be enough to build a quality model and to test its
effectiveness.

## Explore the Data

### The outcome 

Let's look at the variable we are trying to predict--or the "outcome" variable. 
We create tables to show the distribution of students marked dropouts and 
students not marked dropouts.

```{r, echo=TRUE}
table(data$dropout, useNA = "always")
round((table(data$dropout, useNA = "always")/nrow(data))*100,2)

```

About 19% of students are marked as dropouts. We will keep this in mind for our 
analyses, as prediction models can sometimes be insensitive to predicting 
minority outcomes. There is no missing data for our outcome variable.

### Categorical features

Let's use a loop to investigate the distributions of our categorical predictors:

```{r, eval=FALSE}
#Loop over categorical feature names
for(i in c("male", "race_ethnicity", "frpl", "iep", "ell", 
           "gifted","ever_alternative")){
  
  #Print one-way table
  print(i)
  print(table(data[, i], useNA="always"))
  
}#End loop over feature names

```

We have 11 missing values for the gender indicator, 685 missing values 
for the race-ethnicity variable, and 684 missing values for the free/reduced 
lunch indicator. No other values are missing. Note that when we read the
data in, `race_ethnicity` contains blank values that are not marked as `NA` 
by R. Let's change that here:

```{r}
#Replace blanks with 'NA'
data$race_ethnicity[data$race_ethnicity == ""] <- NA
table(data$race_ethnicity, useNA = "always")
```

Finally, we will make sure these categorical variables are classified as factors
in R, so that the program treats them as categorical (rather than ordinal or 
numeric):

```{r MakeFactors}
#Vector of all factor variables
factors <- c("dropout","male","race_ethnicity","gifted",
             "ever_alternative","iep","frpl","gifted","ell")

#Store factor variables as factor in data
#Loop over feature names
for(variable in factors){
  
  #Store as factor
  data[, variable] <- as.factor(as.character(data[, variable]))
  
}# End loop over feature names

```

### Numeric features

Let's look at our continuous or ordinal predictor variables: 

```{r SummaryNumeric}
#Loop over numeric feature names
for(i in c("math_ss","read_ss","pct_days_absent", "gpa")){
  
  print(i)
  print(summary(data[, i])) #Print summary for feature
  
}#End loop over numeric features

```

Each variable contains missing values: the reading and math scores have 
particularly high rates of missingness, with close to 9,000 missing values. 
We also see very high outliers--impossibly high--for all variables besides 
GPA. Let's truncate those by turning them into missing values. Then we'll 
visualize the distributions for each as a histogram:

```{r abs_trunc}
#Loop over numeric feature names
for(i in c("math_ss","read_ss","pct_days_absent","gpa")){
  
  data[,i][data[,i] > 100] <- NA #Truncate impossibly high values
  hist(data[, i], main=i) #Visualize as histogram
  
}#End loop over feature names

```

We see very heavy right skew for percent absent. This makes 
sense, as most students will miss below about 20% of school days.
However, there will be some students who miss much more often, resulting
in high outliers. Cumulative GPA shows left skew, which is also typical:
most students will earn passing grades in a majority of their classes, but
some will have higher failure rates, thus the outliers are on the lower (left)
end of the distribution. The exam score distributions are fairly symmetric.

For many prediction models, working with normally distributed (or 
approximately normal) can make computation smoother and faster. In the next 
section, we transform the skewed variables to make them more symmetric.

### Transformations

Let's start with transforming the right-skewed distribution of 
percent of days absent. The goal is to make this distribution appear
more normal or symmetric. One common strategy is to take the logarithm
of each data point. The value of the logarithm of an extremely high number 
will be considerably less than the original value, whereas the difference 
between the value of a low number and its logarithm is relatively lower in 
magnitude. Thus, while preserving the order of the data points, the logarithm
of a right-skewed distribution with produce lower values for high outliers, 
making the shape more symmetric. We show the logarithm transformation of the 
percent absent data feature here:

```{r DataTransform1}
# // Investigate transformations
#Visualize log transformation of percent absences
hist(log(data$pct_days_absent))

```

Note that it was transformed it too much, making it a left-skewed distribution. 
The log transformation, in other words, was too powerful. Moreover, if we found an 
out-of-sample student with perfect attendance, the log transformation would not work,
since the logarithm of zero is undefined. A more moderate transformation would be 
raising the data to a power below 1. We will try raising the data to the power of 0.5, 
also known as the square root transformation, and to the power of 0.25.

```{r DataTransform2}
# // Investigate transformations
#Visualize square root transformation
hist(sqrt(data$pct_days_absent))

#Visualize transformation of (1/4) power
hist(data$pct_days_absent^0.25)

```

The square root transformation is not powerful enough, but the second 
transformation--raising to the power of 0.25--does the trick. The data looks 
a lot more normal. Thus, we save the data transformed by that power.

```{r DataTransform3}
#Store transformed data
data$pct_days_absent <- data$pct_days_absent^0.25

```

Now let's transform the left-skewed distribution. To transform a 
left-skewed distribution, often squaring the values is a good solution. 
Because GPA is non-negative, the order will be preserved even if we 
square the values.

```{r DataTransform4}
#Visualize square root transformation
hist(data$gpa^2)

```

Although not necessarily normal, the distribution at least appears more 
symmetric now, which will help in our modeling. We store this 
transformed version of the variable.

```{r DataTransform5}
#Store transformed data
data$gpa <- data$gpa^2

```

## Train and Test

In predictive modeling, it is key to create a "test" set of data. 
The test set is data that is not used in the model fitting stage of the 
analysis. In other words, it is data that goes "unseen" by the model 
until we finalize our model. Then, you can "test" your model's predictive 
accuracy by using it to predict the outcomes on the previously unseen 
test set. It's accuracy on the test data provides an estimate of how 
accurate it would be in predicting outcomes for completely new data
(often called "out of sample" data).

The data that is used in the model fitting stage is called the "train" set, 
since it's the data that is used to train your model's parameters. A 
conventional way to split a dataset between train and test sets is to randomly 
choose 80% of the data points to be in the train set, and 20% of the data points 
to be in the test set. This means that the bulk of the information is being used 
to fit the model, which should lead to a more accurate model (think about an 
extreme case: a model that fits around only one data point will not be very 
predictive--the more data, usually, the better). Fewer data points are needed 
to provide an estimate of predictive accuracy, thus the test set is smaller.

Here, we split our data randomly between train and test sets:

```{r TrainTestSplit, echo=TRUE}
#Set seet
set.seed(1738)

#Create a vector of integers, which will be indeces for datasets
indeces <- rownames(data)

#Randomly sort these integers
indeces <- sample(indeces, size = length(indeces), replace = FALSE)

#Define number of data points for train set
n.train <- round(nrow(data)*0.8,0)

#Create train and test sets
train_data <- data[indeces[1:n.train],]
test_data <- data[indeces[(n.train+1):nrow(data)],]
rownames(train_data) <- NULL
rownames(test_data) <- NULL

#Show dimensions of both datasets
dim(train_data)
dim(test_data)

```


## Data Processing

### Standardize and Center

We will standardize our numeric predictors to have a standard deviation of one 
and center them at zero. Doing so helps with rounding considerations and reduces 
the chance of collinearity between predictors. It will also smooth and speed
our computation and allow us to compare predictors on the same scale.

The `caret` package has multiple functions that help with the typical
steps involved in machine learning. Here, the package's `preProcess` function
will allow us to standardize and center our predictors in just two lines.
The function finds the standard deviation and mean for each predictor.
Next, it subtracts each observation by the mean (centering it at zero)
and then divides each value by the standard deviation (to standardize
it).

```{r DataProcess, echo=TRUE}
#Stores objection with informatoin on centering and scaling numeric data
processor <- preProcess(train_data, method = c("center","scale"))

#Centers and scales our training numberic data
train_data_s <- predict(processor, train_data)

```


### Missing Values

Now let's look into handling the datasets missing value, specifically within
the training set.

```{r missing}
summary(train_data_s)

```

It looks like we have missing values for gender, race-ethnicity, 8th grade 
math exam score, 8th grade reading exam score, percent absent, cumulative 
GPA, and free and reduced lunch status. Various imputation and missing 
data handling methods exist, and best practices in machine learning for 
missing data is still an area of active research. 

One simple and computationally efficient way to handle missing data is to 
impute values of central tendency or commonality. For example, for 
continuous variables we would replace the missing values with simply the 
mean of the values for that feature. For categorical variables, we would 
impute the mode. 

However, in the context of predicting dropouts, missing values can hold 
key information. Students who move from home-to-home, switch from school-to-school, 
or miss a lot of school often may have data missing from their file in 
education databases. In addition, the factors that cause a student to have 
missing data may also be good predictors of whether or not that student 
is likely to dropout of school. If we use a simple imputation method to 
get rid of missing values, we could lose that key information. Our final 
model, therefore, would be less accurate in its predictions.

To explore the amount of predictive information a missing value may provide, 
we will run a simple logistic regression on our training dataset. Logistic 
regression provides us an elementary model for binary classification, in 
which we can make a preliminary judgement of the power of each predictor. 

We will create indicator values for missingness, which will become additional 
predictors in our logistic regression model. For example, we will create 
a variable that reads "1" if the student has missing data for their 
8th grade math test, and "0" if the data is present. Then, the size 
of the coefficient for this indicator from our logistic regression 
will tell us if adding this extra variable has a chance to add any predictive 
power to our final model. The reason we can compare the size of the coefficients
is that we've already standardized our predictors, so they operate on the same
scale (standard deviation of 1). If a predictor has a coefficient in our model
that is large in magnitude, that means that a 1 standard deviation change in
that predictor (or, in the case of indicator predictors, a change in indicator
class) can explain a relatively large amount of the variation in the response
variable. 

We implement the logistic regression below, by creating indicator variables for
all features with missing data (0 = not missing, 1 = missing) and by performing
simple mean/mode imputations. :

```{r missing2}
#All our features with missing data
missing.features <-   c("male",
                        "race_ethnicity",
                        "math_ss",
                        "read_ss",
                        "pct_days_absent",
                        "gpa",
                        "frpl")

#Stores copy of our training data
data.miss <- train_data_s

#Loop over features with missing values
for(feature in missing.features){
  
  #Create and attach indicator of missing value
  data.miss[, paste("miss",feature)] <- as.factor(
                                          ifelse(
                                            complete.cases(data.miss[,feature]),
                                                         0,
                                                         1))
  
}#End loop over features with missing values

#Impute missing values to run logistic regression
#Loop over all features
for(feature in colnames(data.miss)){
  
  #If feature is a factor variable
  if(class(data.miss[,feature]) == "factor"){
    
    #Impute the mode for missing values (use helper functino 'Mode')
    data.miss[is.na(data.miss[,feature]) ,feature] <- Mode(data.miss[,feature])
    
  }#End if conditional 
  
  else{
    
    #If non-factor, impute the mean
    data.miss[is.na(data.miss[,feature]) ,feature] <- mean(data.miss[,feature], na.rm = T)
    
  }#End else conditional
  
}#End loop over features

#Fit logistic regression model and summarize
logit <- glm(dropout ~ . , data = data.miss, family = "binomial")
summary(logit)$coef[,c(1,4)]

```

In our basic logistic regression model, some of our predictors that had the 
highest magnitude coefficients were the missingness indicators. In fact, of the 
top 5 highest coefficient values, 4 of them were attached to missing value 
indicators. The highest magnitude coefficient was that of the missing GPA 
predictor. These results show that the missingness contains key information that
could help our final model with predictions. 

Therefore, we will include the missingness indicators as candidate predictors
for our final model, as we have reason to believe their inclusion may help our
model's prediction power. However, we have to be careful: adding too many 
predictors to a prediction model can lead to overfitting and worse prediction 
accuracy on out-of-sample data. We will account for this concern during the 
model fitting stage, in the cross validation section.

As done in our basic logistic regression model, all missing values will be given
a mean or mode imputation, allowing us to be computationally efficient and to 
prevent cutting students from our model fitting or predictions.

```{r missingtrain}
#All our features that will get a missingness indicator
miss.ind.features <-   c("male",
                        "race_ethnicity",
                        "math_ss",
                        "read_ss",
                        "pct_days_absent",
                        "gpa",
                        "frpl")

#Loop over features to include missing indicators for
for(feature in miss.ind.features){
  
  #Create and attach indicator of missing value
  train_data_s[, paste("miss",feature,sep="")] <- as.factor(
                                                ifelse(
                                                  complete.cases(train_data_s[,feature]),
                                                         0,
                                                         1))
  
}#End loop over features 

#Impute missing values
#Loop over all features
for(feature in colnames(train_data_s)){
  
  #If feature is a factor variable
  if(class(train_data_s[,feature]) == "factor"){
    
    #Impute the mode for missing values
    train_data_s[is.na(train_data_s[,feature]) ,feature] <- Mode(train_data_s[,feature])
    
  }#End if conditional 
  
  else{
    
    #If non-factor, impute the mean
    train_data_s[is.na(train_data_s[,feature]) ,feature] <- mean(train_data_s[,feature], 
                                                            na.rm = T)
    
  }#End else conditional
  
}#End loop over features

#Check if any NAs left
summary(train_data_s)
```

### Polynomial Terms

Sometimes, the outcome from a model may vary linearly with
the predictors. Other times, the variation may have a quadratic
or higher-order polynomial relationship. Adding polynomial transformations
of our predictor variables can help better capture this relationship and
could lead to better predictions from the model. 

Again, note: adding too many predictors to a prediction model can lead to
overfitting and worse prediction accuracy on out-of-sample data.
We will account for this concern during the model fitting stage,
in the cross validation section.

Here, we attach additional polynomial transformed features to our
dataset:

```{r PolyTerms}
# // Create polynomial terms
#Loop over feature names
for(variable in c("math_ss","read_ss","pct_days_absent","gpa")){
  
  #Create new column with feature raised to certain power in train set
  train_data_s[, paste(variable,"^2",sep="")] <- train_data_s[, variable]^2
  train_data_s[, paste(variable,"^3",sep="")] <- train_data_s[, variable]^3
  
}# End loop over feature names
```

## Cross-Validation and Model Fit

### Overfitting: An Example

The most important stage of the predictive modeling process is 
cross-validation. Let's walk through a simple example to elucidate why:

Say you are running a regression. There is one predictor variable, X, and there 
is one outcome variable, Y. Here is the data:

<img src="../img/no_fit.png" style="display: block; margin: 0 auto; height: 170px;">

Below, we visualize three possible models one could use to predict Y given X:

<img src="../img/fits.png" style="display: block; margin: 0 auto; height: 170px;">

The three sample regression equations that produce these models are:

A: $Y = {\beta}_{0} + {\beta}_{1}X$

B: $Y = {\beta}_{0} + {\beta}_{1}X + {\beta}_{2}X^{2}$

C: $Y = {\beta}_{0} + {\beta}_{1}X + {\beta}_{2}X^{2} + {\beta}_{3}X^{3} + {\beta}_{4}X^{4} + {\beta}_{5}X^{5} + ...$

Let's evaluate the first model: model A. This model is too simple,
as it tries to map a linear model to a clearly curved relation between
our X and Y values. If we had to predict new out-of-sample data from the 
same population, we wouldn't be confident in our predictions. Particularly
if we were to predict Y values from X values in the center of our distribution,
our predictions would generally be underestimates, so we see that this model 
is biased.

Another attempt we could make at modeling is to add a quadratic predictor,
to better capture the curved trend we see in the data. This is shown as 
model B. Although this model doesn't perfectly "connect the dots" in the given data, we can
be reasonably certain that it will provide relatively reasonable estimates on new
data. It assumes a polynomial shape to the data with some random noise around that 
shape. 

Why  not go for broke and try to perfectly model this data? We can do so by adding 
even more polynomial terms to our set of predictors. Let's look at model C. This
model, which is our most complex in terms the number of predictors,
has no bias on the given data. It perfectly models each point. However, for new data,
we cannot be so confident this model will still "connect the dots." In fact, it will
most likely do a sub-optimal job at predicting new data, since its many twists and turns
are modeling random noise, rather than the underlying trend in the distribution. Unlike 
model A, this model has low bias on our training data; however, because 
it models random noise, it has high variance, which will lead to sub-optimal 
predictions on out-of-sample data.

Model A is "underfit," meaning it has low variance but high bias: it does
not capturing the underlying trend in our data. Underfit models do a poor job modeling
the given training data, and they do a poor job predicting new out-of-sample data.
Model C is "overfit," meaning it has low bias but high variance: it is
attempting to systematically model the random noise in our given data. Overfit models
do an excellent job modeling our given training data, but they do a poor job predicting
new out-of-sample data. 

The goal in making a prediction model is to "tune" our model so that it is not underfit
or overfit. The Lasso provides a way to do this. 

### The Lasso's Penalty

The Lasso binary classification model is based on logistic regression. The danger with
using logistic regression for predicting new data is that we may overfit our model, by
either using too many predictors (as with the example above) or having Beta coefficient
values that are too high in magnitude--that are too strong. The Lasso has a built-in
"tuning parameter", $\lambda$, which acts as a penalty factor on our Betas. If a 
predictor's Beta value is so strong that it will start to cause our model to fit to noise,
it will reduce the Beta value. Or, if a certain feature doesn't really help our model
explain variation in the data enough, it will penalize the Beta to a value of 0, effectively
cutting out that predictor from our model completely.

The trick is that if we make the penalty factor, $\lambda$, too high, it will
penalize our Beta values too much and our model may underfit the data. If we set $\lambda$
too low, the penalty will not be great enough to prevent overfitting. So that begs the
question: how do we find the best possible value for $\lambda$?

### K-fold cross-validation

That is where cross-validation comes in. Here, we take you through K-fold cross-validation.
In K-fold cross validation, we cut our data into 'k' folds. For example, say we had 100
data points and we wanted to do k = 5 folds. We take our data and slice it into 5 equal
portions (so 20 data points in each portion). Then, for a certain value of $\lambda$, 
we fit a Lasso model on the first four portions of the data. The last portion (the last 20
data points) is saved as a mock 'test set'. We see how well our model does in predicting 
values on this test set. We then delegate a different slice of the data to be the new test 
set, and we fit our model on the other four folds. We iterate this process five times 
until each slice of the data has been the test set at least once. Every time we iterate
through one of the k folds, we save the prediction accuracy of the model. Then we average
these accuracies to come up with a predicted accuracy for a model of that $\lambda$. By 
delegating a portion as a test set, this method allows us to estimate prediction accuracy on out-of-sampled data. We then do the above process for a set of $\lambda$ values we'd like
to try. The tuning parameter value that produces the best prediction accuracy in
cross-validation is the value we will use in our final model.

Here is pseudocode that succinctly summarizes the structure of cross-validation:

```
for each tuning parameter value
  for each of the 'k' folds
    fit model (using parameter value) on training sets of fold
    test model prediction accuracy on test set fold
    store prediction accuracy in a vector
  take the average of the accuracy rates over 'k' folds
  store average accuracy rate for the parameter value
select the parameter value that produced the highest accuracy rate

```
Now we manually implement cross-validation on our own training data, using the Lasso.
Note that we are using functions from the `caret` package here to produce the k-folds,
and we are using `glmnet` to fit the Lasso model. 

```{r crossvallambda}
set.seed(4612)

#Creates matrix of covariates
X <- model.matrix(dropout ~ ., data = train_data_s)[,-1]

#5 fold cross validation
K <- 5

#Get indeces for folds
flds <- createFolds(y=train_data_s$dropout, 
                    k=K, list=TRUE, returnTrain=TRUE)
names(flds)[1] <- "training"

#Make range of parameters to test
powers <- seq(-7, 5, 0.25) 
lambdas <- 10**powers

# set up parallel processing
# most computers have 4 cores and this will work across Mac and PC. If more cores 
# are available it can be made faster by increasing this number, but 4 is a good 
# default
plan("multiprocess", workers = 4) 

#Loop over all parameter options
lasso_results <- foreach(d = 1:length(lambdas), .combine = rbind) %dopar% {

 #Stores lambda value for loop run
 l <- lambdas[d]
 
 # Vectors to store accuracies over folds
 accuracies <- vector(mode = "numeric", length=K)
 dropout.accuracies <- vector(mode = "numeric", length=K)
 
 #Loop over K folds
 for(i in 1:K){

   #Sets indeces for values in folds
   train.index <- flds[[i]] #80% of data used in training
   test.index <- setdiff(rownames(train_data_s), flds[[i]]) #20% of data used to test

   #Select training covariates and outcomes
   X.train <- X[train.index,]
   out.train <- train_data_s[train.index, "dropout"]

   #Fit with lambda
   model <- glmnet(x = X.train,
                   y = out.train,
                   family = "binomial", alpha = 1, lambda = l)

   #Select test covariates and outcomes
   X.test <- X[test.index,]
   observed <- train_data_s[test.index, "dropout"]

   #Make predictions on test data
   probabilities <- predict(model, newx = X.test, type = "response")
   predicted <- ifelse(probabilities < mean(probabilities), 1, 0)
   
   # Model accuracy
   accuracies[i] <- mean(predicted == observed) #overall accuracy
   dropout.accuracies[i] <- mean(predicted[observed==1] == observed[observed==1])

 } #End loop over k folds

 #Average accuracies from all folds
 data.frame(lambda = l, 
             overall.accuracy = mean(accuracies), 
            dropout.accuracy = mean(dropout.accuracies))
  
} # End loop over parameter options

plan(sequential) # this cleans up any open R sessions as part of the parallel 
# computing

```

Now that we have run our cross-validation and have gotten prediction accuracies
at every potential value of $\lambda$ we tested, let's visualize our results.
First, we start by visualizing the average overall prediction accuracies for 
each tested parameter value. Overall prediction accuracy is the proportion of 
predictions (dropout or not dropout) the model correctly made on test sets
across the K folds. 

```{r}
#Shows overall accuracy at different lambda values
lineplot.overall <- ggplot(lasso_results, aes(lambda))+
             geom_smooth(aes(y = overall.accuracy), color = "dodgerblue2")+
             scale_x_continuous(trans ='log10')

#Show plot
lineplot.overall

```

Our cross-validation shows that setting the parameter
close to zero, which would be close to equivalent with logistic regression, is
not optimal. By having a robust penalty factor, our model corrects
for some overfitting and can produce a better overall accuracy rate. 

Overall prediction accuracy isn't the only measure of model performance. In
fact, in this case, a model with a relatively high overall prediction accuracy
may be a poor model for the problem we are trying to solve. Since 81% of
students will not dropout, we could make a naive model that predicts all
students to not drop out. This model would have, then, an 81% overall prediction
accuracy on test data (approximately). While this seems high, we know that such
a model is useless, as it would not identify any of the future dropouts. 

To account for this, let's take a look at the parameter values in our
cross-validation and their effectiveness in prediction accuracy only among the
future dropouts. We call this the "dropout accuracy", or the proportion of
actual future dropouts that the model correctly predicted would drop out.

```{r}
#Shows dropout accuracy at different lambda values
lineplot.dropout <- ggplot(lasso_results, aes(lambda))+
             geom_smooth(aes(y = dropout.accuracy), color = "dodgerblue2")+
             scale_x_continuous(trans ='log10')

lineplot.dropout

```

Here, we see an almost completely inverted shape. As the parameter values
increase, the accuracy among dropouts becomes incredibly low; in other words,
the higher the penalty factor, the less sensitive our model gets to predicting
dropouts. Let's compare the best overall accuracy and best dropout dropout 
accuracy during cross-validation, as well as their associated $\lambda$ values.

```{r}
#Stores best overall accuracy
best.overall.accuracy <- round(max(lasso_results$overall.accuracy),2)

#Stores lambda value that created best overall accuracy
best.lambda.overall <- round(lasso_results$lambda[lasso_results$overall.accuracy == 
                                    max(lasso_results$overall.accuracy)],8)
best.lambda.overall <- best.lambda.overall[length(best.lambda.overall)] #In case of repeats

#Stores dropout accuracy at lambda value that created best overall accuracy
dropoutacc.best.overall <- round(lasso_results$dropout.accuracy[lasso_results$overall.accuracy == 
                                    max(lasso_results$overall.accuracy)],2)

dropoutacc.best.overall <- dropoutacc.best.overall[length(dropoutacc.best.overall)] #Repeats

#Stores best dropout accuracy
best.dropout.accuracy <- round(max(lasso_results$dropout.accuracy),2)

#Stores lambda value that created best dropout accuracy
best.lambda.dropout <- round(lasso_results$lambda[lasso_results$dropout.accuracy == 
                                    max(lasso_results$dropout.accuracy)],8)
best.lambda.dropout <- best.lambda.dropout[length(best.lambda.dropout)] #In case of repeats

#Stores overall accuracy at lambda value that created best dropout accuracy
overallacc.best.dropout <- round(lasso_results$overall.accuracy[lasso_results$dropout.accuracy == 
                                    max(lasso_results$dropout.accuracy)],2)

overallacc.best.dropout <- overallacc.best.dropout[length(overallacc.best.dropout)] #Repeats

#Create data frame of best values
cross.results <- data.frame(overall_accuracy = c(best.overall.accuracy,
                                                overallacc.best.dropout),
                            dropout_accuracy = c(dropoutacc.best.overall,
                                                 best.dropout.accuracy),
                            lambda_value = c(best.lambda.overall,
                                             best.lambda.dropout))

rownames(cross.results) <- c("Best Overall Accuracy",
                             "Best Dropout Accuracy")

#Show results
head(cross.results)
```

Our cross-validation shows us the value of the Lasso's penalty 
factor, $\lambda$, that maximized overall prediction accuracy across the 5-fold 
test sets: `r best.lambda.overall`. The accuracy this tuning parameter value 
produced was `r best.overall.accuracy`. However, at this parameter value, our 
accuracy among dropouts was `r dropoutacc.best.overall`. This model approaches 
the naive model of just predicting each student to not drop out and is,
effectively, useless. 

By contrast, our best performance among the dropouts was an accuracy of only 
`r best.dropout.accuracy` at a lambda value of `r best.lambda.dropout`. At this
lambda value, the overall accuracy was only `r overallacc.best.dropout`. There
is a clear trade-off: the more complex models (with lower parameter values) seem
to be more able to predict dropouts, but they have poor overall accuracy. The
models with high penalty factors are too simple and are useless at picking 
dropouts, even if they do have high overall accuracy. 

We need to take into account the fact that one class (dropouts) has far fewer 
data points than the other class (non-dropouts). How can we fit our model so 
that it's more sensitive to the minority class (dropouts) while also making sure
it doesn't provide too many false alarms (i.e., predicting a dropout when the 
student wouldn't have otherwise dropped out)? The next section will discuss this
issue.

Note: Above, we implemented the cross-validation manually, in order to show 
the structure of cross-validation, However, this is rarely done in 
practice. The `caret` package includes useful tools for training models 
via cross-validation. We demonstrate the same cross-validation process below, 
using functions from `caret`.

```{r CaretKFold}
#Set k for k-fold validation
K = 5

#Prepare outcome data as a factor with different level lables
train_data_car <- train_data_s
train_data_car$dropout <- as.factor(as.character(train_data_car$dropout))
levels(train_data_car$dropout) <- c("X0","X1")

#Set seed
set.seed(4612)

#Sets type of cross-validation in training fucntion
TC <- trainControl(method = "cv",
                   number = K, allowParallel = TRUE)

plan(multisession, workers = 4) # create parallel workers to speed things up
#Trains ands fits model using cross-validation
lasso.model <- train(dropout~., train_data_car, method = "glmnet",
                     tuneGrid = expand.grid(.alpha = 1,
                                             .lambda = 10**seq(-7,7, by = 0.1)),
                     trControl = TC,
                     metric = "Accuracy",
                     family = "binomial")

plan(sequential) # close out parallel workers

##Shows best tuning parameter in model
#lasso.model$bestTune["lambda"]

##Shows performance of model
#getTrainPerf(lasso.model)

#Shows coefficients in model
coef(lasso.model$finalModel, lasso.model$bestTune$lambda)

```

When training a model in the `caret` implementation of cross-validation, it is
easy to see the parameter value that produced the model with the highest 
overall classification accuracy and to see what the highest classification 
accuracy actually was. We commented out lines in the code above that would allow
the user to see those outputs. 

It is also easy to see the coefficients of the model that produced the highest
classification accuracy during cross-validation, and these coefficients are 
shown in the output above. Note that the Lasso tuned some of these coefficient 
values to zero, meaning the predictors were effectively dropped from our model. 
Thus, we can see how the Lasso's penalty factor not only diminishes coefficient
values, but also drops some variables from the model completely, in order to 
simplify and prevent overfitting.

### Unbalanced Classes

Our cross-validation was run trying to find the $\lambda$ value that maximized
overall classification accuracy. In doing so, we ran the danger of using a tuning
parameter value that would maximize overall accuracy, at the cost of predictive
power among future dropouts. We can tell from the output of the cross-validation
that our chosen model would do pretty horrendously in predicting future
dropouts.

We need to use a method other than overall classification accuracy, which takes 
into account our classification goals. Our goals are two-fold: 

1. Maximize true positives: We want to classify, at high rates, 
future dropouts as 'dropouts.' If our model fails to identify future dropouts,
it has failed in its mission. Planned interventions will not reach the
students who need it.

2. Minimize false positives: We want to minimize, as much as possible,
instances in which a non-dropout is predicted to be a dropout by our model. 
Interventions for likely dropout students can be costly--if our model tells
policymakers to target students with dropout interventions and those students
are not students who otherwise would have dropped out, then we create costly 
inefficiency. 

### The ROC Curve

A metric that takes both of our goals into account is the receiver operating
characteristic, or the ROC. To understand the ROC, it helps to think through how
the Lasso, and classification models like it, make predictions.

The Lasso does not automatically output predictions. Rather, it outputs
probabilities. For example, our model may predict that a certain student's 
probability of dropping out is 40%. It's then up to the user to determine if we 
want to predict this person to be a dropout or a non-dropout. The probability
threshold at which we declare a "dropout" is known as the 'cutoff.'

If we set our cutoff low, then the criteria for our model to declare an observation
as a dropout is less strenuous. As a result, we will have a higher true positive 
rate. At the same time, we will make more mistakes, creating more false positives. 
By the same token, raising the cutoff creates fewer false positives, but it also creates
fewer true positives. This creates a continuum of true and false positive rates based
on cutoff values that is often visualized as the 'ROC curve'. Here is an example
of an ROC curve:

<img src="../img/roc_example.png" style="display: block; margin: 0 auto; height: 380px;">

The labeled points on the curve are cutpoints. As the cutpoints decrease in value, the
true positive rate increases, as does the false positive rate. As the cutpoints increase
in value, the true positive rate decreases in value, as does the false positive rate. 
The goal of the model is to push this curve as far to the upper-left corner of the graph
as possible: to the area with high true positive rate and low false positive rate. So,
we want to perform a cross-validation to tune our $\lambda$ to the value that produces the 
best (the most shifted upwards and left) ROC curve. A way to distill the "best" ROC curve
is to take the area under the curve, also known as the 'AUC.' The higher the AUC, the more
shifted (and stretched) our ROC curve is to the upward-left corner of the graph. The highest
AUC model will have cutpoints that that produce the lowest false positive rates and highest
true positive rates. 

So, we will run another cross-validation, this time using AUC as the model evaluation
metric, rather than overall classification accuracy. Here is pseudocode for the process:

```
for each tuning parameter value
  for each of the 'k' folds
    fit model (using parameter value) on training sets of fold
    test model AUC on test set fold
    store AUC in a vector
  take the average of the AUC over 'k' folds
  store average AUC for the parameter value
select the parameter value that produced the highest average AUC

```
Note that this can be done again in the `caret` package, with slight modifications:

### Fitting with ROC

```{r ROCFit}
#Set seed
set.seed(4612)

#Set k for k-fold validation
K = 5

#Gives function output and cross-validation tuning
TC <- trainControl(method = "cv",
                   number = K,
                   classProbs = TRUE,
                   savePredictions = TRUE,
                   allowParallel = TRUE,
                   summaryFunction = twoClassSummary)

plan(multisession, workers = 4)
#Trains model with method AUC
model.ROC <- train(dropout~., train_data_car, method = "glmnet",
              metric = "ROC",
              tuneGrid = expand.grid(.alpha = 1,
                                     .lambda = 10**seq(-7,7, by = 0.1)),
              trControl = TC)
plan(sequential)

#Best paramter chosen
model.ROC$bestTune["lambda"]

#Shows performance on subgroups and ROC
getTrainPerf(model.ROC)

```

Our cross-validation shows us the value of the Lasso's penalty 
factor, $\lambda$, that maximized AUC across the 5-fold test sets: 
`r model.ROC$bestTune[["lambda"]]`. The AUC this tuning parameter value 
produced was `r round(model.ROC$bestTune[["lambda"]], 2)`. 

Let's take a look at the structure of the model that produced the highest AUC:

```{r}
#Shows coefficients in model
coef(model.ROC$finalModel, model.ROC$bestTune$lambda)

```

This is the final model we will use to make predictions on out-of-sample test
data. Note that here, just as above, the Lasso's penalty nixed several
predictors from our model, in order to simplify it and prevent overfitting.

The ROC curve generated from this model during cross-validation is visualized
below.

```{r}
#Get indeces for best model lambda
roc.index <- model.ROC$pred$lambda == model.ROC$bestTune[["lambda"]]

#Plot ROC curve for best model
ggplot(model.ROC$pred[roc.index, ], 
        aes(m = X1, d = factor(obs, levels = c("X0", "X1"))))+ 
        geom_roc(hjust = -0.4, 
                 vjust = 1.5, 
                 n.cuts = 13,
                 labelround = 2)+ 
        coord_equal()+
        ylab("True Positive Rate")+
        xlab("False Positive Rate")+
        ggtitle("ROC Curve on Training Data")

```

So, with this cross-validation, we have tuned $\lambda$ and fitted our model.
We can already see that this model will perform better than the extreme models
we produced during our first run at cross-validation. At various cutpoints in
the ROC curve above above, we have a fairly high dropout accuracy (true positive
rate), and the amount of false alarms (false positives) will not be absurd.

Now, we just need to estimate our accuracies on the out-of-sample test set.

## Test Set Prediction

### Process Test Data

Just as we processed the training data by removing missing values, adding predictors, and
centering/standardizing features, we will do the same to our test data. It is important to
only use values for mean/mode imputation and standardization that were determined during 
training, as we are trying to mimic our model's performance on true out-of-sample prediction.

We can start by centering and standardizing.  

```{r DataProcesstest, echo=TRUE}
#Centers and scales our testing numberic data (based on train data)
test_data_s <- predict(processor, test_data)

```

We can now add a missingness indicator and impute missing values.

```{r missingtest}
#Loop over features to include missing indicators for
for(feature in miss.ind.features){
  
  #Create and attach indicator of missing value
  test_data_s[, paste("miss",feature,sep="")] <- as.factor(
                                                ifelse(
                                                  complete.cases(test_data_s[,feature]),
                                                         0,
                                                         1))
  
}#End loop over features 

#Impute missing values (using values from train set)
#Loop over all features
for(feature in colnames(test_data_s)){
  
  #If feature is a factor variable
  if(class(test_data_s[,feature]) == "factor"){
    
    #Impute the mode for missing values
    test_data_s[is.na(test_data_s[,feature]) ,feature] <- Mode(train_data_s[,feature])
    
  }#End if conditional 
  
  else{
    
    #If non-factor, impute the mean
    test_data_s[is.na(test_data_s[,feature]) ,feature] <- mean(train_data_s[,feature], 
                                                            na.rm = T)
    
  }#End else conditional
  
}#End loop over features

#Check if any NAs left
summary(test_data_s)
```

Now we can add polynomial predictors.

```{r PolyTermstest}
#Store factor variables as factor in data
#Loop over feature names
for(variable in c("math_ss","read_ss","pct_days_absent","gpa")){
  
  #Create new column with feature raised to certain power in test set
  test_data_s[, paste(variable,"^2",sep="")] <- test_data_s[, variable]^2
  test_data_s[, paste(variable,"^3",sep="")] <- test_data_s[, variable]^3
  
}# End loop over feature names

```

### Making Predictions

Finally, we make our predictions on the test data. We will generate our final ROC curve
to evaluate our out-of-sample performance at various cutpoints. 

```{r Prediction}
#Predicts probabilities of dropping out on test data
pred.probs <- predict(model.ROC, newdata = test_data_s[, -1], type = "prob")

#Stores actual test data outcomes (and re-level)
observed <- test_data_s[, "dropout"]
levels(observed) <- c("X0","X1")

#Make into dataframe fro plotting
ROC.plot.frame <- data.frame(X1 = pred.probs$X1,
                             obs = factor(observed, levels = c("X0","X1")))

#Plot ROC curve for test set
ggplot(ROC.plot.frame, 
        aes(m = X1, d = obs))+ 
        geom_roc(hjust = -0.4, 
                 vjust = 1.5, 
                 n.cuts = 13,
                 labelround = 2)+ 
        coord_equal()+
        ylab("True Positive Rate")+
        xlab("False Positive Rate")+
        ggtitle("ROC Curve on Test Data")


```

As expected, this ROC curve will have a slightly worse AUC than the ROC 
produced during training. Even with taking measures to prevent overfitting,
models generally will do better on training data than testing data. So, we
now have a good set of estimates of different accuracy rates at various 
cutpoints for new, out-of-sample data.

### Determining a Cutpoint

The last step is determining an optimal cutpoint. To determine a cutpoint, 
the context of the problem is key. In the case of predicting dropouts, 
most districts would probably value maximizing true positives over 
minimizing false negatives. In other words, districts would rather err on the 
side of over-predicting dropouts, rather than err on the side of
under-predicting them.

If the goal is to minimize the number of dropouts, giving a dropout 
intervention to a non-dropout student (a false positive) has far fewer 
consequences than failing to provide interventions to a student who would 
otherwise drop out (a false negative). In addition, a student who 
receives a drop out intervention will most likely benefit from that 
intervention, regardless of whether or not they would have dropped 
out without it. By contrast,a student who fails to receive a dropout 
intervention and drops out of school will likely face great hardships 
because of it. Therefore, if our model will be biased either towards 
predicting more dropouts than there actually are or predicting fewer 
dropouts than there actually are, we'd want it to over-predict dropouts. 

However, false positives have costs. It'd be impossible to give costly
interventions to all students. A data scientist would need to meet with
policymakers and stakeholders to collectively decide the cutpoint at which
the district would be comfortable providing dropout intervention services.

To inform this discussion, it's often helpful to frame the problem in terms
of raw numbers, rather than probabilities. For example, say you work in a district 
of 1,000 rising 9th graders. The data shows that 19% of rising 9th graders in the 
district drop out. Imagine that for your model, the cutpoint of 10% yielded a 75% 
true positive rate and 25% false positive rate on the test set. You can present 
to stakeholders the following:

"In our district, typically 19% of rising 9th graders eventually drop out of high
school. So, among our 1,000 current rising 9th graders, we'd expect $0.19 * 1,000 = 190$
to drop out. My model tells us the probability that a certain rising 9th grader will
drop out. Our task is to decide when this probability gets high enough that we start
giving an intervention. 

If we decide that we should give interventions to students with a probability of dropping
out of 10% or higher, that would mean that about $0.75 * 190 = 143$ of the 190 future 
dropouts would receive the intervention. In addition, of the 810 non-dropouts, about
$0.25 * 810 = 203$ of them would receive the intervention. So, in total, we'd have about
346 students receiving the dropout intervention.

If we raise the probability threshold above 10%, we'd reduce costs by giving the intervention
to less students. But we'd also catch fewer future dropouts with the intervention."

Using the same calculations above, the data scientist can give multiple numerical estimates
for different cutpoints to district leaders. Finally, a decision would be made. And, just
like that, you now have a finely tuned model to predict future dropouts!
